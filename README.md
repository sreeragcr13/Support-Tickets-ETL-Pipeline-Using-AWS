# Support Tickets ETL Pipelines using AWS 
Support Tickets ETL Pipelines is a cloud-based data engineering project built for the fictional BPO company CarePlus.


# AWS ETL Pipeline Project

## ğŸ“‹ Project Overview

This project demonstrates a complete end-to-end ETL (Extract, Transform, Load) pipeline built on AWS cloud infrastructure. The pipeline processes both log files and CSV data, transforming raw data from an OLTP system into a queryable data warehouse with interactive dashboards.

## ğŸ—ï¸ Architecture

The project implements a modern data engineering architecture using AWS services:

- **Data Extraction**: OLTP data ingestion into AWS S3 Data Lake
- **Data Transformation**: Serverless processing using AWS Lambda and AWS Glue
- **Data Warehousing**: Amazon Redshift for analytics-ready data storage
- **Data Analytics**: AWS Athena for ad-hoc queries and Power BI for visualization

## ğŸ› ï¸ Technologies Used

- **AWS S3**: Data Lake storage for raw and processed data
- **AWS Lambda**: Serverless compute for log file transformations
- **AWS Glue**: ETL service for CSV data processing
- **Amazon Athena**: Serverless SQL query engine
- **Amazon Redshift**: Cloud data warehouse
- **Power BI**: Business intelligence and dashboard creation
- **Python**: Data transformation logic

## âœ¨ Key Features

- **Automated Data Ingestion**: S3 event-driven triggers for real-time processing
- **Scalable ETL Pipeline**: Serverless architecture that scales automatically
- **Dual Processing Paths**: 
  - Lambda for log file transformations
  - Glue for CSV file processing
- **Incremental Loading**: Efficient data warehouse updates
- **Cost Optimization**: Budget alerts and serverless computing
- **Interactive Dashboards**: Power BI visualizations connected to Redshift

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw OLTP data
â”‚   â””â”€â”€ processed/              # Transformed data
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ log_processor.py        # Lambda function for log files
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ csv_etl_job.py         # Glue job for CSV processing
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ athena_queries.sql     # Sample Athena queries
â””â”€â”€ docs/
    â””â”€â”€ architecture.png        # Architecture diagram
```

## ğŸš€ Implementation Steps

### 1. Data Extraction & Storage
- Set up AWS account and configured IAM permissions
- Created S3 buckets for data lake (raw, processed, curated zones)
- Implemented data ingestion pipeline from OLTP source to S3

### 2. Data Transformation

#### Log Files ETL
- Developed Python transformation logic locally
- Created Lambda function for log file processing
- Configured manual triggers for testing
- Implemented automated S3 event triggers for production

#### CSV Files ETL
- Designed Glue ETL jobs for CSV data processing
- Set up Glue Data Catalog for metadata management
- Automated ETL execution using S3 event notifications

### 3. Data Warehousing & Analytics
- Configured AWS Athena for ad-hoc SQL queries
- Set up Amazon Redshift cluster
- Loaded transformed data from S3 into Redshift
- Implemented incremental data loading strategy
- Built interactive Power BI dashboards connected to Redshift

## ğŸ“Š Data Flow
```
OLTP System â†’ S3 (Raw) â†’ Lambda/Glue (Transform) â†’ S3 (Processed) â†’ Redshift â†’ Power BI
                                                  â†“
                                              Athena (Ad-hoc Queries)
```

## ğŸ’¡ Key Learnings

- **Lambda vs Glue**: Understanding when to use each service based on data volume and complexity
- **Event-Driven Architecture**: Implementing real-time data processing with S3 triggers
- **Cost Management**: Setting up budget alerts and monitoring AWS resource usage
- **Incremental Loading**: Efficient strategies for updating data warehouse without full reloads
- **Serverless Benefits**: Automatic scaling and pay-per-use pricing model

## ğŸ”§ Setup Instructions

### Prerequisites
- AWS Account
- Python 3.x
- Power BI Desktop
- Basic understanding of SQL

### Configuration Steps

1. **Clone the repository**
```bash
   git clone https://github.com/yourusername/aws-etl-pipeline.git
   cd aws-etl-pipeline
```

2. **Set up AWS credentials**
```bash
   aws configure
```

3. **Create S3 buckets**
   - Raw data bucket
   - Processed data bucket
   - Glue scripts bucket

4. **Deploy Lambda function**
   - Package Python dependencies
   - Upload to AWS Lambda
   - Configure S3 trigger

5. **Configure Glue job**
   - Create Glue crawler
   - Set up ETL job
   - Configure S3 event trigger

6. **Set up Redshift**
   - Create Redshift cluster
   - Configure security groups
   - Load data from S3

7. **Connect Power BI**
   - Install Redshift ODBC driver
   - Configure data source
   - Build dashboards

## ğŸ“ˆ Results

- Successfully processed [X GB] of data daily
- Reduced query time by [X%] using Redshift vs traditional databases
- Automated pipeline reduced manual processing time by [X hours/week]
- Built [X] interactive dashboards for business insights

## ğŸ’° Cost Optimization

- Implemented AWS Budget alerts for cost monitoring
- Used serverless architecture to minimize idle resource costs
- Optimized Glue job concurrency and worker allocation
- Configured Redshift cluster pause/resume schedules

## ğŸ”® Future Enhancements

- [ ] Implement data quality checks using AWS Glue Data Quality
- [ ] Add AWS Step Functions for complex workflow orchestration
- [ ] Integrate Amazon QuickSight as an alternative BI tool
- [ ] Implement data versioning and lineage tracking
- [ ] Add real-time streaming with Amazon Kinesis
- [ ] Set up CI/CD pipeline for automated deployments

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

Your Name - [your.email@example.com](mailto:your.email@example.com)

Project Link: [https://github.com/yourusername/aws-etl-pipeline](https://github.com/yourusername/aws-etl-pipeline)

## ğŸ™ Acknowledgments

- AWS Documentation
- Data Engineering Community
- [Any other resources or inspirations]
